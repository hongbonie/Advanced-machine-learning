{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "################# Load and Save Data ################\n",
    "\n",
    "def load_json(rfdir, rfname):\n",
    "    with codecs.open(join(rfdir, rfname), 'r', encoding='utf-8') as rf:\n",
    "        return json.load(rf)\n",
    "\n",
    "\n",
    "def dump_json(obj, wfpath, wfname, indent=None):\n",
    "    with codecs.open(join(wfpath, wfname), 'w', encoding='utf-8') as wf:\n",
    "        json.dump(obj, wf, ensure_ascii=False, indent=indent)\n",
    "\n",
    "\n",
    "\n",
    "def dump_data(obj, wfpath, wfname):\n",
    "    with open(os.path.join(wfpath, wfname), 'wb') as wf:\n",
    "        pickle.dump(obj, wf)\n",
    "\n",
    "\n",
    "def load_data(rfpath, rfname):\n",
    "    with open(os.path.join(rfpath, rfname), 'rb') as rf:\n",
    "        return pickle.load(rf)\n",
    "\n",
    "    \n",
    "################# Random Walk ################\n",
    "\n",
    "import random\n",
    "class MetaPathGenerator:\n",
    "    def __init__(self):\n",
    "        self.paper_author = dict()\n",
    "        self.author_paper = dict()\n",
    "        self.paper_org = dict()\n",
    "        self.org_paper = dict()\n",
    "        self.paper_conf = dict()\n",
    "        self.conf_paper = dict()\n",
    "\n",
    "    def read_data(self, dirpath):\n",
    "        temp=set()\n",
    "\n",
    "        with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "            for line in pafile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_org:\n",
    "                        self.paper_org[p] = []\n",
    "                    self.paper_org[p].append(a)\n",
    "                    if a not in self.org_paper:\n",
    "                        self.org_paper[a] = []\n",
    "                    self.org_paper[a].append(p)\n",
    "        temp.clear()\n",
    "\n",
    "              \n",
    "        with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "            for line in pafile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_author:\n",
    "                        self.paper_author[p] = []\n",
    "                    self.paper_author[p].append(a)\n",
    "                    if a not in self.author_paper:\n",
    "                        self.author_paper[a] = []\n",
    "                    self.author_paper[a].append(p)\n",
    "        temp.clear()\n",
    "        \n",
    "                \n",
    "        with open(dirpath + \"/paper_conf.txt\", encoding='utf-8') as pcfile:\n",
    "            for line in pcfile:\n",
    "                temp.add(line)                       \n",
    "        for line in temp: \n",
    "                toks = line.strip().split(\"\\t\")\n",
    "                if len(toks) == 2:\n",
    "                    p, a = toks[0], toks[1]\n",
    "                    if p not in self.paper_conf:\n",
    "                        self.paper_conf[p] = []\n",
    "                    self.paper_conf[p].append(a)\n",
    "                    if a not in self.conf_paper:\n",
    "                        self.conf_paper[a] = []\n",
    "                    self.conf_paper[a].append(p)\n",
    "        temp.clear()\n",
    "                    \n",
    "        print (\"#papers \", len(self.paper_conf))      \n",
    "        print (\"#authors\", len(self.author_paper))\n",
    "        print (\"#org_words\", len(self.org_paper))\n",
    "        print (\"#confs  \", len(self.conf_paper)) \n",
    "    \n",
    "    def generate_WMRW(self, outfilename, numwalks, walklength):\n",
    "        outfile = open(outfilename, 'w')\n",
    "        for paper0 in self.paper_conf: \n",
    "            for j in range(0, numwalks): #wnum walks\n",
    "                paper=paper0\n",
    "                outline = \"\"\n",
    "                i=0\n",
    "                while(i<walklength):\n",
    "                    i=i+1    \n",
    "                    if paper in self.paper_author:\n",
    "                        authors = self.paper_author[paper]\n",
    "                        numa = len(authors)\n",
    "                        authorid = random.randrange(numa)\n",
    "                        author = authors[authorid]\n",
    "                        \n",
    "                        papers = self.author_paper[author]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper           \n",
    "                        \n",
    "                    if paper in self.paper_org:\n",
    "                        words = self.paper_org[paper]\n",
    "                        numw = len(words)\n",
    "                        wordid = random.randrange(numw) \n",
    "                        word = words[wordid]\n",
    "                    \n",
    "                        papers = self.org_paper[word]\n",
    "                        nump = len(papers)\n",
    "                        if nump >1:\n",
    "                            paperid = random.randrange(nump)\n",
    "                            paper1 = papers[paperid]\n",
    "                            while paper1 == paper:\n",
    "                                paperid = random.randrange(nump)\n",
    "                                paper1 = papers[paperid]\n",
    "                            paper = paper1\n",
    "                            outline += \" \" + paper  \n",
    "                            \n",
    "                outfile.write(outline + \"\\n\")\n",
    "        outfile.close()\n",
    "        \n",
    "        print (\"walks done\")\n",
    "        \n",
    "################# Compare Lists ################\n",
    "\n",
    "def tanimoto(p,q):\n",
    "    c = [v for v in p if v in q]\n",
    "    return float(len(c) / (len(p) + len(q) - len(c)))\n",
    "\n",
    "\n",
    "\n",
    "################# Paper similarity ################\n",
    "\n",
    "def generate_pair(pubs,outlier): ##求匹配相似度\n",
    "    dirpath = 'gene'\n",
    "    \n",
    "    paper_org = {}\n",
    "    paper_conf = {}\n",
    "    paper_author = {}\n",
    "    paper_word = {}\n",
    "    \n",
    "    temp=set()\n",
    "    with open(dirpath + \"/paper_org.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_org:\n",
    "                paper_org[p] = []\n",
    "            paper_org[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_conf.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_conf:\n",
    "                paper_conf[p]=[]\n",
    "            paper_conf[p]=a\n",
    "    temp.clear()\n",
    "    \n",
    "    with open(dirpath + \"/paper_author.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_author:\n",
    "                paper_author[p] = []\n",
    "            paper_author[p].append(a)\n",
    "    temp.clear()\n",
    "       \n",
    "    with open(dirpath + \"/paper_word.txt\", encoding='utf-8') as pafile:\n",
    "        for line in pafile:\n",
    "            temp.add(line)                       \n",
    "    for line in temp: \n",
    "        toks = line.strip().split(\"\\t\")\n",
    "        if len(toks) == 2:\n",
    "            p, a = toks[0], toks[1]\n",
    "            if p not in paper_word:\n",
    "                paper_word[p] = []\n",
    "            paper_word[p].append(a)\n",
    "    temp.clear()\n",
    "    \n",
    "    \n",
    "    paper_paper = np.zeros((len(pubs),len(pubs)))\n",
    "    for i,pid in enumerate(pubs):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        for j,pjd in enumerate(pubs):\n",
    "            if j==i:\n",
    "                continue\n",
    "            ca=0\n",
    "            cv=0\n",
    "            co=0\n",
    "            ct=0\n",
    "          \n",
    "            if pid in paper_author and pjd in paper_author:\n",
    "                ca = len(set(paper_author[pid])&set(paper_author[pjd]))*1.5\n",
    "            if pid in paper_conf and pjd in paper_conf and 'null' not in paper_conf[pid]:\n",
    "                cv = tanimoto(set(paper_conf[pid]),set(paper_conf[pjd]))\n",
    "            if pid in paper_org and pjd in paper_org:\n",
    "                co = tanimoto(set(paper_org[pid]),set(paper_org[pjd]))\n",
    "            if pid in paper_word and pjd in paper_word:\n",
    "                ct = len(set(paper_word[pid])&set(paper_word[pjd]))/3\n",
    "                    \n",
    "            paper_paper[i][j] =ca+cv+co+ct\n",
    "            \n",
    "    return paper_paper\n",
    "\n",
    "    \n",
    "        \n",
    "################# Evaluate ################\n",
    "        \n",
    "def pairwise_evaluate(correct_labels,pred_labels):\n",
    "    TP = 0.0  # Pairs Correctly Predicted To SameAuthor\n",
    "    TP_FP = 0.0  # Total Pairs Predicted To SameAuthor\n",
    "    TP_FN = 0.0  # Total Pairs To SameAuthor\n",
    "\n",
    "    for i in range(len(correct_labels)):\n",
    "        for j in range(i + 1, len(correct_labels)):\n",
    "            if correct_labels[i] == correct_labels[j]:\n",
    "                TP_FN += 1\n",
    "            if pred_labels[i] == pred_labels[j]:\n",
    "                TP_FP += 1\n",
    "            if (correct_labels[i] == correct_labels[j]) and (pred_labels[i] == pred_labels[j]):\n",
    "                TP += 1\n",
    "\n",
    "    if TP == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = TP / TP_FP\n",
    "        pairwise_recall = TP / TP_FN\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "    return pairwise_precision, pairwise_recall, pairwise_f1\n",
    "\n",
    "\n",
    "################# Save Paper Features ################\n",
    "\n",
    "def save_relation(name_pubs_raw, name): # 保存论文的各种feature\n",
    "    name_pubs_raw = load_json('genename', name_pubs_raw)\n",
    "    ## trained by all text in the datasets. Training code is in the cells of \"train word2vec\"\n",
    "    save_model_name = \"word2vec/Aword2vec.model\"\n",
    "    model_w = word2vec.Word2Vec.load(save_model_name)\n",
    "    \n",
    "    r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "    stopword = ['at','based','in','of','for','on','and','to','an','using','with','the','by','we','be','is','are','can']\n",
    "    stopword1 = ['university','univ','china','department','dept','laboratory','lab','school','al','et',\n",
    "                 'institute','inst','college','chinese','beijing','journal','science','international']\n",
    "    \n",
    "    f1 = open ('gene/paper_author.txt','w',encoding = 'utf-8')\n",
    "    f2 = open ('gene/paper_conf.txt','w',encoding = 'utf-8')\n",
    "    f3 = open ('gene/paper_word.txt','w',encoding = 'utf-8')\n",
    "    f4 = open ('gene/paper_org.txt','w',encoding = 'utf-8')\n",
    "\n",
    "    \n",
    "    taken = name.split(\"_\")\n",
    "    name = taken[0] + taken[1]\n",
    "    name_reverse = taken[1]  + taken[0]\n",
    "    if len(taken)>2:\n",
    "        name = taken[0] + taken[1] + taken[2]\n",
    "        name_reverse = taken[2]  + taken[0] + taken[1]\n",
    "    \n",
    "    authorname_dict={}\n",
    "    ptext_emb = {}  \n",
    "    \n",
    "    tcp=set()  \n",
    "    for i,pid in enumerate(name_pubs_raw):\n",
    "        \n",
    "        pub = name_pubs_raw[pid]\n",
    "        \n",
    "        #save authors\n",
    "        org=\"\"\n",
    "        for author in pub[\"authors\"]:\n",
    "            authorname = re.sub(r,'', author[\"name\"]).lower()\n",
    "            taken = authorname.split(\" \")\n",
    "            if len(taken)==2: ##检测目前作者名是否在作者词典中\n",
    "                authorname = taken[0] + taken[1]\n",
    "                authorname_reverse = taken[1]  + taken[0] \n",
    "            \n",
    "                if authorname not in authorname_dict:\n",
    "                    if authorname_reverse not in authorname_dict:\n",
    "                        authorname_dict[authorname]=1\n",
    "                    else:\n",
    "                        authorname = authorname_reverse \n",
    "            else:\n",
    "                authorname = authorname.replace(\" \",\"\")\n",
    "            \n",
    "            if authorname!=name and authorname!=name_reverse:\n",
    "                f1.write(pid + '\\t' + authorname + '\\n')\n",
    "        \n",
    "            else:\n",
    "                if \"org\" in author:\n",
    "                    org = author[\"org\"]\n",
    "                    \n",
    "                    \n",
    "        #save org 待消歧作者的机构名\n",
    "        pstr = org.strip()\n",
    "        pstr = pstr.lower() #小写\n",
    "        pstr = re.sub(r,' ', pstr) #去除符号\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip() #去除多余空格\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr=set(pstr)\n",
    "        for word in pstr:\n",
    "            f4.write(pid + '\\t' + word + '\\n')\n",
    "\n",
    "        \n",
    "        #save venue\n",
    "        pstr = pub[\"venue\"].strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        for word in pstr:\n",
    "            f2.write(pid + '\\t' + word + '\\n')\n",
    "        if len(pstr)==0:\n",
    "            f2.write(pid + '\\t' + 'null' + '\\n')\n",
    "\n",
    "            \n",
    "        #save text\n",
    "        pstr = \"\"    \n",
    "        keyword=\"\"\n",
    "        if \"keywords\" in pub:\n",
    "            for word in pub[\"keywords\"]:\n",
    "                keyword=keyword+word+\" \"\n",
    "        pstr = pstr + pub[\"title\"]\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>1]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        for word in pstr:\n",
    "            f3.write(pid + '\\t' + word + '\\n')\n",
    "        \n",
    "        #save all words' embedding\n",
    "        pstr = keyword + \" \" + pub[\"title\"] + \" \" + pub[\"venue\"] + \" \" + org\n",
    "        if \"year\" in pub:\n",
    "              pstr = pstr +  \" \" + str(pub[\"year\"])\n",
    "        pstr=pstr.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        pstr = pstr.split(' ')\n",
    "        pstr = [word for word in pstr if len(word)>2]\n",
    "        pstr = [word for word in pstr if word not in stopword]\n",
    "        pstr = [word for word in pstr if word not in stopword1]\n",
    "\n",
    "        words_vec=[]\n",
    "        for word in pstr:\n",
    "            if (word in model_w):\n",
    "                words_vec.append(model_w[word])\n",
    "        if len(words_vec)<1:\n",
    "            words_vec.append(np.zeros(100))\n",
    "            tcp.add(i)\n",
    "            #print ('outlier:',pid,pstr)\n",
    "        ptext_emb[pid] = np.mean(words_vec,0)\n",
    "        \n",
    "    #  ptext_emb: key is paper id, and the value is the paper's text embedding\n",
    "    dump_data(ptext_emb,'gene','ptext_emb.pkl')\n",
    "    # the paper index that lack text information\n",
    "    dump_data(tcp,'gene','tcp.pkl')\n",
    "            \n",
    " \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from os.path import join\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "pubs_raw = load_json(\"train\",\"train_pub.json\")\n",
    "pubs_raw1 = load_json(\"sna_data\",\"sna_valid_pub.json\")\n",
    "r = '[!“”\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~—～’]+'\n",
    "f1 = open ('result/all_text.txt','w',encoding = 'utf-8')\n",
    "\n",
    "for i,pid in enumerate(pubs_raw):\n",
    "    pub = pubs_raw[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    venue = pub[\"venue\"]\n",
    "    pstr=venue.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "for i,pid in enumerate(pubs_raw1):\n",
    "    pub = pubs_raw1[pid]\n",
    "    \n",
    "    for author in pub[\"authors\"]:\n",
    "        if \"org\" in author:\n",
    "                org = author[\"org\"]\n",
    "                pstr = org.strip()\n",
    "                pstr = pstr.lower()\n",
    "                pstr = re.sub(r,' ', pstr)\n",
    "                pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "                f1.write(pstr+'\\n')\n",
    "            \n",
    "    title = pub[\"title\"]\n",
    "    pstr=title.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "    if \"abstract\" in pub and type(pub[\"abstract\"]) is str:\n",
    "        abstract = pub[\"abstract\"]\n",
    "        pstr=abstract.strip()\n",
    "        pstr = pstr.lower()\n",
    "        pstr = re.sub(r,' ', pstr)\n",
    "        pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "        f1.write(pstr+'\\n')\n",
    "        \n",
    "    venue = pub[\"venue\"]\n",
    "    pstr=venue.strip()\n",
    "    pstr = pstr.lower()\n",
    "    pstr = re.sub(r,' ', pstr)\n",
    "    pstr = re.sub(r'\\s{2,}', ' ', pstr).strip()\n",
    "    f1.write(pstr+'\\n')\n",
    "    \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec \n",
    "\n",
    "sentences = word2vec.Text8Corpus(r\"result/all_text.txt\")\n",
    "model = word2vec.Word2Vec(sentences, size=100, negative=5, min_count=2, window=5)\n",
    "model.save(\"word2vec/Aword2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 li_guo 1464\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'genename/li_guo.json'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-aaff6284b263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mname_pubs_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpubs_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdump_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_pubs_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'genename'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0msave_relation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m###############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-32472427711d>\u001b[0m in \u001b[0;36mdump_json\u001b[0;34m(obj, wfpath, wfname, indent)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdump_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'genename/li_guo.json'"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "pubs_raw = load_json(\"train\",\"train_pub.json\")\n",
    "name_pubs = load_json(\"train\",\"train_author.json\")\n",
    "\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "result=[]\n",
    "for n,name in enumerate(name_pubs):\n",
    "    ilabel=0\n",
    "    pubs=[] # all papers\n",
    "    labels=[] # ground truth\n",
    "    \n",
    "    for author in name_pubs[name]:\n",
    "        iauthor_pubs = name_pubs[name][author]\n",
    "        for pub in iauthor_pubs:\n",
    "            pubs.append(pub)\n",
    "            labels.append(ilabel)\n",
    "        ilabel += 1\n",
    "        \n",
    "    print (n,name,len(pubs))\n",
    "    \n",
    "    \n",
    "    if len(pubs)==0:\n",
    "        result.append(0)\n",
    "        continue\n",
    "    \n",
    "    ##保存关系\n",
    "    ###############################################################\n",
    "    name_pubs_raw = {}\n",
    "    for i,pid in enumerate(pubs):\n",
    "        name_pubs_raw[pid] = pubs_raw[pid]\n",
    "        \n",
    "    dump_json(name_pubs_raw, 'genename', name+'.json', indent=4)\n",
    "    save_relation(name+'.json', name)  \n",
    "    ###############################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##元路径游走类\n",
    "    ###############################################################r\n",
    "    mpg = MetaPathGenerator()\n",
    "    mpg.read_data(\"gene\")\n",
    "    ###############################################################\n",
    "    \n",
    "  \n",
    "    \n",
    "    ##论文关系表征向量\n",
    "    ############################################################### \n",
    "    all_embs=[]\n",
    "    rw_num =3\n",
    "    cp=set()\n",
    "    for k in range(rw_num):\n",
    "        mpg.generate_WMRW(\"gene/RW.txt\",5,20) #生成路径集\n",
    "        sentences = word2vec.Text8Corpus(r'gene/RW.txt')\n",
    "        model = word2vec.Word2Vec(sentences, size=100,negative =25, min_count=1, window=10)\n",
    "        embs=[]\n",
    "        for i,pid in enumerate(pubs):\n",
    "            if pid in model:\n",
    "                embs.append(model[pid])\n",
    "            else:\n",
    "                cp.add(i)\n",
    "                embs.append(np.zeros(100))\n",
    "        all_embs.append(embs)\n",
    "    all_embs= np.array(all_embs)\n",
    "    print ('relational outlier:',cp)\n",
    "    ############################################################### \n",
    "\n",
    "    \n",
    "    \n",
    "    ##论文文本表征向量\n",
    "    ###############################################################   \n",
    "    ptext_emb=load_data('gene','ptext_emb.pkl')\n",
    "    tcp=load_data('gene','tcp.pkl')\n",
    "    print ('semantic outlier:',tcp)\n",
    "    tembs=[]\n",
    "    for i,pid in enumerate(pubs):\n",
    "        tembs.append(ptext_emb[pid])\n",
    "    ############################################################### \n",
    "    \n",
    "    ##离散点\n",
    "    outlier=set()\n",
    "    for i in cp:\n",
    "        outlier.add(i)\n",
    "    for i in tcp:\n",
    "        outlier.add(i)\n",
    "    \n",
    "    ##网络嵌入向量相似度\n",
    "    sk_sim = np.zeros((len(pubs),len(pubs)))\n",
    "    for k in range(rw_num):\n",
    "        sk_sim = sk_sim + pairwise_distances(all_embs[k],metric=\"cosine\")\n",
    "    sk_sim =sk_sim/rw_num    \n",
    "    \n",
    "    ##文本相似度\n",
    "    t_sim = pairwise_distances(tembs,metric=\"cosine\")\n",
    "    \n",
    "    w=1\n",
    "    sim = (np.array(sk_sim) + w*np.array(t_sim))/(1+w)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##evaluate\n",
    "    ###############################################################\n",
    "    pre = DBSCAN(eps = 0.2, min_samples = 4,metric =\"precomputed\").fit_predict(sim)\n",
    "    \n",
    "    \n",
    "    for i in range(len(pre)):\n",
    "        if pre[i]==-1:\n",
    "            outlier.add(i)\n",
    "    \n",
    "    ## assign each outlier a label\n",
    "    paper_pair = generate_pair(pubs,outlier)\n",
    "    paper_pair1 = paper_pair.copy()\n",
    "    K = len(set(pre))\n",
    "    for i in range(len(pre)):\n",
    "        if i not in outlier:\n",
    "            continue\n",
    "        j = np.argmax(paper_pair[i])\n",
    "        while j in outlier:\n",
    "            paper_pair[i][j]=-1\n",
    "            j = np.argmax(paper_pair[i])\n",
    "        if paper_pair[i][j]>=1.5:\n",
    "            pre[i]=pre[j]\n",
    "        else:\n",
    "            pre[i]=K\n",
    "            K=K+1\n",
    "    \n",
    "    ## find nodes in outlier is the same label or not\n",
    "    for ii,i in enumerate(outlier):\n",
    "        for jj,j in enumerate(outlier):\n",
    "            if jj<=ii:\n",
    "                continue\n",
    "            else:\n",
    "                if paper_pair1[i][j]>=1.5:\n",
    "                    pre[j]=pre[i]\n",
    "            \n",
    "            \n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    pre = np.array(pre)\n",
    "    print (labels,len(set(labels)))\n",
    "    print (pre,len(set(pre)))\n",
    "    pairwise_precision, pairwise_recall, pairwise_f1 = pairwise_evaluate(labels,pre)\n",
    "    print (pairwise_precision, pairwise_recall, pairwise_f1)\n",
    "    result.append(pairwise_f1)\n",
    "\n",
    "    print ('avg_f1:', np.mean(result))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}